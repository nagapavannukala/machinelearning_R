---
title: "Text Analytics"
author: "Kathirmani Sukumar"
date: "7/30/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Creating corpus
```{r}
tweets = read.csv('/Users/skathirmani/Documents/datasets/narendramodi_tweets.csv')
amazon = read.csv('/Users/skathirmani/Documents/datasets/amazon_reviews.csv')
hotstar = read.csv('/Users/skathirmani/Documents/datasets/hotstar.allreviews_Sentiments.csv')

dim(hotstar)
```


```{r}
docs = as.character(tweets$text)
library(tm)

docs = VCorpus(VectorSource(docs))
docs = tm_map(docs, content_transformer(tolower))# Convert all characters to lower case
inspect(docs[[1]])
docs = tm_map(docs, stripWhitespace) # Remove additional white space
```

### Document Term Matrix
```{r}
dtm = DocumentTermMatrix(docs)
dim(dtm)
df_dtm = as.data.frame(as.matrix(dtm))
dim(dtm)
View(df_dtm[1:100,1:50])
colnames(df_dtm)[1:50]
View(colnames(df_dtm))
```

# Get top 50 words
```{r}
x = colSums(df_dtm)
words_freq = data.frame(words=labels(x),
                        freq=x)

words_freq %>% arrange(-freq) %>% head(50)
```

```{r}
common_stopwords = stopwords()
length(common_stopwords)
custom_stopwords = c('&amp;')

all_stop_words = c(common_stopwords, custom_stopwords)
length(all_stop_words)

docs = tm_map(docs, removeWords, all_stop_words)
dtm = DocumentTermMatrix(docs)
df_dtm = as.data.frame(as.matrix(dtm))
x = colSums(df_dtm)
words_freq = data.frame(words=labels(x),
                        freq=x)

words_freq %>% arrange(-freq) %>% head(50)

```

### Regular expressions
```{r}
s = 'this is a Sentence with numbers 123 & special chars'
gsub('a', '', s)
gsub('[0-9]', '', s) # Remove numbers
gsub('[a-z]', '', s) # Remove alphabets with lower case
gsub('[a-zA-Z]', '', s) # Remove alphabets with all case
gsub('[a-zA-Z0-9]', '', s) # Remove alphabets & numbers
gsub('[^a-zA-Z]', '', s)
gsub('[^a-zA-Z #@]', '', s)
```

# Building proper corpus
```{r}
library(tm)
library(dplyr)
regex_func = function(x){ return (gsub('[^a-z #@]', '', x)) }
custom_stopwords = c('amp', 'will', ' @narendramodi', ' @pmoindia')
common_stopwords = stopwords()

all_stop_words = c(custom_stopwords, common_stopwords)
docs = as.character(amazon$reviewText)
docs = VCorpus(VectorSource(docs))
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, content_transformer(regex_func))
docs = tm_map(docs, stripWhitespace)
docs = tm_map(docs, removeWords, all_stop_words)
dtm = DocumentTermMatrix(docs)
df_dtm = as.data.frame(as.matrix(dtm))
x = colSums(df_dtm)
words_freq = data.frame(words=labels(x),
                        freq=x)
words_freq %>% filter(! words %in% all_stop_words) %>% arrange(-freq) %>% head(50)
```

### Wordclouds
```{r}
library(wordcloud)
library(RColorBrewer)
wordcloud(words_freq$words, 
          words_freq$freq, 
          min.freq = 100, 
          random.order = F,
          colors = brewer.pal(name='Accent',8),
          random.color = T)

```

### Document length
```{r}
docs_length = rowSums(df_dtm)
```



## Sentiment Analysis
```{r}
library(RSentiment)
calculate_sentiment(c('i hate apple', 'i love india', 'this is a sentence'))
```

### Supervised Sentiment Analysis
```{r}
dim(df_dtm)
top_words = words_freq %>% filter(!words %in% c('@hotstartweets', 'app', 'hotstar','@hotstarhelps') ) %>% arrange(-freq) %>% head(25)
df_dtm_subset = df_dtm[,top_words$words]
df_dtm_subset$sentiment = hotstar$Sentiment_Manual
View(df_dtm_subset)
```

## Build Naive Bayes Model
```{r}
library(e1071)
train = df_dtm_subset[1:3000, ]
test = df_dtm_subset[3001:nrow(df_dtm_subset),]

model_naive = naiveBayes(sentiment~., data=train)
test$predict_sentiment = predict(model_naive, test%>%select(-sentiment))
table(test$predict_sentiment, test$sentiment)
```

```{r}
(44+4+1266) / nrow(test) * 100
```

### Trending of sentiment
```{r, fig.width=12, fig.height=5}
library(ggplot2)
hotstar$Created_Date = as.Date(hotstar$Created_Date, format='%m/%d/%Y')
hotstar %>% 
  group_by(Created_Date, Sentiment_Manual) %>% 
  summarise(count=n()) %>% 
  ggplot(aes(x=Created_Date, y=count, group=Sentiment_Manual, color=Sentiment_Manual)) + geom_line()
```

```{r, fig.width=5, fig.height=3}
ggplot(hotstar, aes(x=Sentiment_Manual)) + geom_bar()
```


### Bigram Analysis
```{r}
# sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib

Sys.setenv(JAVA_HOME='C:/Program Files/Java/jre-9.0.4')
library(RWeka)
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
dtm_bigram = DocumentTermMatrix(docs, control=list(tokenize=BigramTokenizer))
df_dtm_bigram = as.data.frame(as.matrix(dtm_bigram))
colnames(df_dtm_bigram)[1:10]
x = colSums(df_dtm_bigram)
bigrams_words_freq = data.frame(words=labels(x),
                                freq=x)
bigrams_words_freq %>% arrange(-freq) %>% head(10)
```



```{r}
bigrams_negative = c()
bigrams_positive = c()
negative_words = c('poor', 'waste', 'bad', 'not',
                   'defective', 'disgusting', 'dont', 'untrusty',
                   'worst', 'horrible', 'unexpectedly')
positive_words = c('good', 'awesome', 'worth', 'like', 'best', 'nice',
                  'amazing', 'fantastic' )
for (bigram in bigrams_words_freq$words){
  words = unlist(strsplit(bigram, ' '))
  match_negatives = intersect(negative_words, words)
  match_positives = intersect(positive_words, words)
  if (length(match_negatives)>0){
    bigrams_negative = c(bigrams_negative, bigram)
  }
  if (length(match_positives)>0){
    bigrams_positive = c(bigrams_positive, bigram)
  }
}
bigrams_negative
x = colSums(df_dtm_bigram[,bigrams_negative])
bigrams_negative_freq = data.frame(words=labels(x),
                                   freq=x)
bigrams_negative_freq %>% arrange(-freq) %>% head(20)
View(bigrams_negative_freq)
```

## Word similarity
```{r}
library(lsa)
cosine(df_dtm[,'book'], df_dtm[,'kindle']) # Similarity between book and kindle
cosine(as.vector(df_dtm[100,]), as.vector(df_dtm[300,])) # Similarity between 100th and 300th document

```

```{r}
words_similar = function(word, df_dtm){
  
}
words_similar('tablet', df_dtm)
```

```{r}
library(tidytext)
library(topicmodels)
dtm_nonsparse = removeSparseTerms(dtm, sparse=0.95)
dtm
dtm_nonsparse = dtm_nonsparse[rowSums(as.matrix(dtm_nonsparse))>0,]
lda.out = LDA(dtm_nonsparse, 4, method='Gibbs')
word2topic = tidy(lda.out, matrix='beta')
View(doc2topic)
doc2topic = tidy(lda.out, matrix='gamma')

topic_top5_words = word2topic %>% group_by(topic) %>% 
  arrange(topic,-beta) %>% 
  top_n(5)
library(ggplot2)
ggplot(topic_top5_words, aes(x=reorder(term, beta), y=beta))+ 
  geom_bar(stat = 'identity') +
  coord_flip() + 
  facet_wrap(~topic, scales = 'free')
```

